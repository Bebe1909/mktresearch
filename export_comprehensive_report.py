#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Word Export for Layered Research Results
Há»— trá»£ xuáº¥t bÃ¡o cÃ¡o vá»›i Layer 3 vÃ  comprehensive Layer 4
Enhanced with tables and executive summary
"""

import json
import os
from datetime import datetime
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.table import WD_TABLE_ALIGNMENT
from docx.oxml.shared import OxmlElement, qn
import re

def get_vietnamese_market_name(market: str) -> str:
    """Translate market name to Vietnamese for consistent Vietnamese reports"""
    market_translations = {
        "ğŸ‡ºğŸ‡¸ United States": "ğŸ‡ºğŸ‡¸ Hoa Ká»³",
        "ğŸ‡¨ğŸ‡³ China": "ğŸ‡¨ğŸ‡³ Trung Quá»‘c", 
        "ğŸ‡¯ğŸ‡µ Japan": "ğŸ‡¯ğŸ‡µ Nháº­t Báº£n",
        "ğŸ‡°ğŸ‡· South Korea": "ğŸ‡°ğŸ‡· HÃ n Quá»‘c",
        "ğŸ‡¹ğŸ‡­ Thailand": "ğŸ‡¹ğŸ‡­ ThÃ¡i Lan",
        "ğŸ‡¸ğŸ‡¬ Singapore": "ğŸ‡¸ğŸ‡¬ Singapore",
        "ğŸ‡²ğŸ‡¾ Malaysia": "ğŸ‡²ğŸ‡¾ Malaysia",
        "ğŸ‡®ğŸ‡© Indonesia": "ğŸ‡®ğŸ‡© Indonesia",
        "ğŸ‡µğŸ‡­ Philippines": "ğŸ‡µğŸ‡­ Philippines",
        "ğŸ‡¬ğŸ‡§ United Kingdom": "ğŸ‡¬ğŸ‡§ VÆ°Æ¡ng quá»‘c Anh",
        "ğŸ‡©ğŸ‡ª Germany": "ğŸ‡©ğŸ‡ª Äá»©c",
        "ğŸ‡«ğŸ‡· France": "ğŸ‡«ğŸ‡· PhÃ¡p",
        "ğŸ‡®ğŸ‡¹ Italy": "ğŸ‡®ğŸ‡¹ Ã",
        "ğŸ‡ªğŸ‡¸ Spain": "ğŸ‡ªğŸ‡¸ TÃ¢y Ban Nha",
        "ğŸ‡¨ğŸ‡¦ Canada": "ğŸ‡¨ğŸ‡¦ Canada",
        "ğŸ‡¦ğŸ‡º Australia": "ğŸ‡¦ğŸ‡º Ãšc",
        "ğŸ‡³ğŸ‡¿ New Zealand": "ğŸ‡³ğŸ‡¿ New Zealand",
        "ğŸ‡§ğŸ‡· Brazil": "ğŸ‡§ğŸ‡· Brazil",
        "ğŸ‡²ğŸ‡½ Mexico": "ğŸ‡²ğŸ‡½ Mexico",
        "ğŸ‡®ğŸ‡³ India": "ğŸ‡®ğŸ‡³ áº¤n Äá»™",
        "ğŸ‡·ğŸ‡º Russia": "ğŸ‡·ğŸ‡º Nga",
        "ğŸ‡¿ğŸ‡¦ South Africa": "ğŸ‡¿ğŸ‡¦ Nam Phi",
        "ğŸ‡ªğŸ‡¬ Egypt": "ğŸ‡ªğŸ‡¬ Ai Cáº­p",
        "ğŸ‡¦ğŸ‡ª UAE": "ğŸ‡¦ğŸ‡ª UAE",
        "ğŸ‡¸ğŸ‡¦ Saudi Arabia": "ğŸ‡¸ğŸ‡¦ áº¢ Ráº­p Saudi",
        "ğŸ‡¹ğŸ‡· Turkey": "ğŸ‡¹ğŸ‡· Thá»• NhÄ© Ká»³",
        "ğŸ‡³ğŸ‡± Netherlands": "ğŸ‡³ğŸ‡± HÃ  Lan",
        "ğŸ‡¸ğŸ‡ª Sweden": "ğŸ‡¸ğŸ‡ª Thá»¥y Äiá»ƒn",
        "ğŸ‡³ğŸ‡´ Norway": "ğŸ‡³ğŸ‡´ Na Uy",
        "ğŸ‡©ğŸ‡° Denmark": "ğŸ‡©ğŸ‡° Äan Máº¡ch",
        "ğŸ‡«ğŸ‡® Finland": "ğŸ‡«ğŸ‡® Pháº§n Lan",
        "ğŸ‡¨ğŸ‡­ Switzerland": "ğŸ‡¨ğŸ‡­ Thá»¥y SÄ©",
        "ğŸ‡¦ğŸ‡¹ Austria": "ğŸ‡¦ğŸ‡¹ Ão",
        "ğŸ‡§ğŸ‡ª Belgium": "ğŸ‡§ğŸ‡ª Bá»‰",
        "ğŸ‡µğŸ‡± Poland": "ğŸ‡µğŸ‡± Ba Lan",
        "ğŸ‡¨ğŸ‡¿ Czech Republic": "ğŸ‡¨ğŸ‡¿ Cá»™ng hÃ²a SÃ©c",
        "ğŸ‡­ğŸ‡º Hungary": "ğŸ‡­ğŸ‡º Hungary",
        "ğŸ‡¬ğŸ‡· Greece": "ğŸ‡¬ğŸ‡· Hy Láº¡p",
        "ğŸ‡µğŸ‡¹ Portugal": "ğŸ‡µğŸ‡¹ Bá»“ ÄÃ o Nha",
        "ğŸ‡®ğŸ‡ª Ireland": "ğŸ‡®ğŸ‡ª Ireland",
        "ğŸ‡®ğŸ‡± Israel": "ğŸ‡®ğŸ‡± Israel",
        "ğŸ‡­ğŸ‡° Hong Kong": "ğŸ‡­ğŸ‡° Há»“ng KÃ´ng",
        "ğŸ‡¹ğŸ‡¼ Taiwan": "ğŸ‡¹ğŸ‡¼ ÄÃ i Loan",
        "ğŸ‡¦ğŸ‡· Argentina": "ğŸ‡¦ğŸ‡· Argentina",
        "ğŸ‡¨ğŸ‡± Chile": "ğŸ‡¨ğŸ‡± Chile",
        "ğŸ‡¨ğŸ‡´ Colombia": "ğŸ‡¨ğŸ‡´ Colombia",
        "ğŸ‡µğŸ‡ª Peru": "ğŸ‡µğŸ‡ª Peru",
        "ğŸ‡»ğŸ‡ª Venezuela": "ğŸ‡»ğŸ‡ª Venezuela",
        "ğŸ‡ªğŸ‡¨ Ecuador": "ğŸ‡ªğŸ‡¨ Ecuador",
        "ğŸ‡ºğŸ‡¾ Uruguay": "ğŸ‡ºğŸ‡¾ Uruguay",
        "ğŸ‡§ğŸ‡´ Bolivia": "ğŸ‡§ğŸ‡´ Bolivia",
        "ğŸ‡µğŸ‡¾ Paraguay": "ğŸ‡µğŸ‡¾ Paraguay",
        "ğŸ‡³ğŸ‡¬ Nigeria": "ğŸ‡³ğŸ‡¬ Nigeria",
        "ğŸ‡°ğŸ‡ª Kenya": "ğŸ‡°ğŸ‡ª Kenya",
        "ğŸ‡¬ğŸ‡­ Ghana": "ğŸ‡¬ğŸ‡­ Ghana",
        "ğŸ‡ªğŸ‡¹ Ethiopia": "ğŸ‡ªğŸ‡¹ Ethiopia",
        "ğŸ‡ºğŸ‡¬ Uganda": "ğŸ‡ºğŸ‡¬ Uganda",
        "ğŸ‡¹ğŸ‡¿ Tanzania": "ğŸ‡¹ğŸ‡¿ Tanzania",
        "ğŸ‡¿ğŸ‡¼ Zimbabwe": "ğŸ‡¿ğŸ‡¼ Zimbabwe",
        "ğŸŒ Southeast Asia": "ğŸŒ ÄÃ´ng Nam Ã",
        "ğŸŒ Asia-Pacific": "ğŸŒ ChÃ¢u Ã - ThÃ¡i BÃ¬nh DÆ°Æ¡ng",
        "ğŸŒ Global Market": "ğŸŒ Thá»‹ trÆ°á»ng ToÃ n cáº§u"
    }
    
    return market_translations.get(market, market)

def add_custom_styles(doc):
    """ThÃªm custom styles cho document"""
    
    # Style cho tiÃªu Ä‘á» chÃ­nh
    try:
        title_style = doc.styles.add_style('CustomTitle', 1)  # 1 = PARAGRAPH
        title_font = title_style.font
        title_font.name = 'Times New Roman'
        title_font.size = Pt(20)
        title_font.bold = True
        title_font.color.rgb = RGBColor(0, 51, 102)
        title_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER
        title_style.paragraph_format.space_after = Pt(20)
    except:
        pass  # Style Ä‘Ã£ tá»“n táº¡i
    
    # Style cho layer heading
    try:
        layer_style = doc.styles.add_style('LayerStyle', 1)
        layer_font = layer_style.font
        layer_font.name = 'Times New Roman'
        layer_font.size = Pt(16)
        layer_font.bold = True
        layer_font.color.rgb = RGBColor(46, 134, 171)
        layer_style.paragraph_format.space_before = Pt(15)
        layer_style.paragraph_format.space_after = Pt(10)
    except:
        pass

def set_paragraph_font(paragraph, font_name='Times New Roman', font_size=11):
    """Set font cho paragraph"""
    for run in paragraph.runs:
        run.font.name = font_name
        run.font.size = Pt(font_size)

def create_info_table(doc, data):
    """Táº¡o báº£ng thÃ´ng tin tá»•ng quan"""
    print("ğŸ“Š Táº¡o báº£ng thÃ´ng tin tá»•ng quan...")
    
    # TÃ­nh toÃ¡n statistics
    stats = calculate_statistics(data)
    
    # Get metadata from research_metadata (correct location)
    metadata = data.get('research_metadata', {})
    
    # Táº¡o table 2 cá»™t
    table = doc.add_table(rows=0, cols=2)
    table.style = 'Table Grid'
    table.alignment = WD_TABLE_ALIGNMENT.CENTER
    
    # ThÃªm rows - get data from metadata
    info_data = [
        ('ğŸ¯ NgÃ nh nghiÃªn cá»©u', metadata.get('industry', 'N/A')),
        ('ğŸŒ Thá»‹ trÆ°á»ng', metadata.get('market', 'N/A')),
        ('ğŸ¤– AI Engine', f"{metadata.get('api_provider', 'N/A')} - {metadata.get('model_used', 'N/A')}"),
        ('ğŸ“… NgÃ y táº¡o', datetime.now().strftime('%d/%m/%Y %H:%M')),
        ('â“ Tá»•ng sá»‘ questions', str(stats['total_questions']))
    ]
    
    for label, value in info_data:
        row = table.add_row()
        row.cells[0].text = label
        row.cells[1].text = value
        
        # Format cells
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                set_paragraph_font(paragraph, font_size=10)
                if cell == row.cells[0]:  # Label cell
                    paragraph.runs[0].bold = True
    
    return table

def create_summary_table(doc, data):
    """Táº¡o báº£ng tÃ³m táº¯t káº¿t quáº£ nghiÃªn cá»©u"""
    print("ğŸ“Š Táº¡o báº£ng tÃ³m táº¯t káº¿t quáº£...")
    
    # Táº¡o table vá»›i headers
    table = doc.add_table(rows=1, cols=4)
    table.style = 'Table Grid'
    table.alignment = WD_TABLE_ALIGNMENT.CENTER
    
    # Headers
    headers = ['Layer/Category', 'Questions', 'Layer 3 Analysis', 'Layer 4 Enhanced']
    header_row = table.rows[0]
    for i, header in enumerate(headers):
        header_row.cells[i].text = header
        # Bold headers
        for paragraph in header_row.cells[i].paragraphs:
            set_paragraph_font(paragraph, font_size=10)
            paragraph.runs[0].bold = True
    
    # Add data rows
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            questions = category.get('questions', [])
            
            layer3_count = len(questions)
            layer4_count = sum(1 for q in questions if q.get('layer4_comprehensive_report'))
            
            row = table.add_row()
            row.cells[0].text = f"{layer_name} / {category_name}"
            row.cells[1].text = str(layer3_count)
            row.cells[2].text = "âœ…" if layer3_count > 0 else "âŒ"
            row.cells[3].text = f"âœ… ({layer4_count})" if layer4_count > 0 else "âŒ"
            
            # Format cells
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    set_paragraph_font(paragraph, font_size=9)
    
    return table

def extract_key_insights(data):
    """TrÃ­ch xuáº¥t key insights Ä‘á»ƒ táº¡o executive summary"""
    insights = []
    
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            
            for question in category.get('questions', []):
                # Láº¥y tá»« comprehensive report trÆ°á»›c
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    content = layer4_comprehensive.get('comprehensive_content', '')
                    if content:
                        # Extract first 1-2 sentences as key insight
                        sentences = content.split('.')[:2]
                        if sentences:
                            insight = '. '.join(sentences).strip()
                            if len(insight) > 50:  # Only meaningful insights
                                insights.append({
                                    'category': f"{layer_name} - {category_name}",
                                    'insight': insight[:200] + "..." if len(insight) > 200 else insight
                                })
                # Fallback to layer 3
                elif question.get('layer3_content'):
                    content = question.get('layer3_content', '')
                    sentences = content.split('.')[:1]
                    if sentences:
                        insight = sentences[0].strip()
                        if len(insight) > 50:
                            insights.append({
                                'category': f"{layer_name} - {category_name}",
                                'insight': insight[:150] + "..." if len(insight) > 150 else insight
                            })
    
    return insights[:8]  # Top 8 insights

def create_references_section(doc, data):
    """Táº¡o pháº§n references cho bÃ¡o cÃ¡o dá»±a trÃªn nguá»“n thá»±c tá»« research"""
    
    # Page break before references
    doc.add_page_break()
    
    # Title
    ref_title = doc.add_heading('ğŸ“š TÃ€I LIá»†U THAM KHáº¢O', level=1)
    ref_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(ref_title, font_size=16)
    
    # Add spacing
    doc.add_paragraph()
    
    # Get research metadata
    metadata = data.get('research_metadata', {})
    topic = metadata.get('industry', 'NghiÃªn cá»©u thá»‹ trÆ°á»ng')
    market = metadata.get('market', 'Viá»‡t Nam')
    vietnamese_market = get_vietnamese_market_name(market)  # Translate to Vietnamese
    model_used = metadata.get('model_used', 'gpt-3.5-turbo')
    api_provider = metadata.get('api_provider', 'OpenAI')
    current_year = datetime.now().year
    
    # Start with AI source acknowledgment
    references = [
        f"1. {api_provider} {model_used}. ({current_year}). AI-powered market research analysis for {topic} in {vietnamese_market}. Retrieved from https://openai.com"
    ]
    
    # Get tracked references from research data
    tracked_references = data.get('tracked_references', [])
    
    if tracked_references:
        # Use real tracked references
        ref_counter = 2
        for source, frequency in tracked_references[:10]:  # Top 10 most cited
            # Clean up source name - remove extra text that might be added by tracking
            clean_source = source.strip()
            
            # Create professional reference format
            references.append(f"{ref_counter}. {clean_source}. ({current_year}). Market research data and analysis. Industry intelligence source.")
            ref_counter += 1
    else:
        # Fallback to generic sources if no tracking data
        fallback_sources = [
            f"2. General Statistics Office (GSO). ({current_year}). Economic and social statistics. Retrieved from https://gso.gov.vn",
            f"3. World Bank. ({current_year}). World Development Indicators. Retrieved from https://data.worldbank.org",
            f"4. Vietnam Chamber of Commerce and Industry (VCCI). ({current_year}). Business environment reports. Retrieved from https://vcci.com.vn"
        ]
        references.extend(fallback_sources)
    
    # Add references to document
    for ref in references:
        ref_para = doc.add_paragraph(ref)
        ref_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        ref_para.paragraph_format.left_indent = Inches(0.3)
        ref_para.paragraph_format.hanging_indent = Inches(0.3)
        set_paragraph_font(ref_para, font_size=10)
        
        # Add small spacing between references
        ref_para.paragraph_format.space_after = Pt(3)
    
    # Add note about data sources
    doc.add_paragraph()
    note_para = doc.add_paragraph()
    note_para.add_run("Ghi chÃº vá» nguá»“n dá»¯ liá»‡u: ").bold = True
    note_para.add_run(f"BÃ¡o cÃ¡o nÃ y Ä‘Æ°á»£c táº¡o báº±ng AI ({api_provider} {model_used}) Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  tá»•ng há»£p thÃ´ng tin vá» thá»‹ trÆ°á»ng {topic} táº¡i {vietnamese_market}. "
                     f"AI Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thu tháº­p, phÃ¢n tÃ­ch vÃ  trÃ¬nh bÃ y thÃ´ng tin tá»« cÃ¡c nguá»“n cÃ´ng khai. "
                     f"CÃ¡c nguá»“n tham kháº£o Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»± Ä‘á»™ng tá»« quÃ¡ trÃ¬nh phÃ¢n tÃ­ch vÃ  Ä‘Æ°á»£c sáº¯p xáº¿p theo táº§n suáº¥t sá»­ dá»¥ng. "
                     f"CÃ¡c thÃ´ng tin vÃ  sá»‘ liá»‡u trong bÃ¡o cÃ¡o pháº£n Ã¡nh kiáº¿n thá»©c vÃ  dá»¯ liá»‡u cÃ³ sáºµn cá»§a mÃ´ hÃ¬nh AI táº¡i thá»i Ä‘iá»ƒm táº¡o bÃ¡o cÃ¡o ({datetime.now().strftime('%m/%Y')}). "
                     f"NgÆ°á»i Ä‘á»c nÃªn xÃ¡c minh thÃ´ng tin vá»›i cÃ¡c nguá»“n chÃ­nh thá»©c vÃ  cáº­p nháº­t Ä‘á»ƒ cÃ³ dá»¯ liá»‡u má»›i nháº¥t.")
    note_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(note_para, font_size=9)
    note_para.runs[0].italic = True
    note_para.runs[1].italic = True

def extract_sources_from_research(research_results):
    """TrÃ­ch xuáº¥t cÃ¡c nguá»“n Ä‘Æ°á»£c Ä‘á» cáº­p trong ná»™i dung research"""
    sources = set()
    
    # Common organizations and sources that might be mentioned in AI responses
    source_patterns = [
        # Vietnamese official sources
        r'Tá»•ng cá»¥c Thá»‘ng kÃª(?:\s+Viá»‡t\s+Nam)?',
        r'Bá»™ (?:Káº¿ hoáº¡ch|TÃ i chÃ­nh|CÃ´ng ThÆ°Æ¡ng|Y táº¿|GiÃ¡o dá»¥c)',
        r'VCCI|Vietnam Chamber of Commerce',
        r'FPT|Viettel|VNPT',
        r'NgÃ¢n hÃ ng NhÃ  nÆ°á»›c',
        
        # International sources
        r'World Bank|NgÃ¢n hÃ ng Tháº¿ giá»›i',
        r'IMF|International Monetary Fund',
        r'ADB|Asian Development Bank',
        r'McKinsey|Deloitte|PwC|KPMG',
        r'Nielsen|Euromonitor',
        r'Statista',
        
        # Industry-specific
        r'VAMA|Vietnam Automobile',
        r'VINASA|Vietnam Software',
        r'VFA|Vietnam Food Association'
    ]
    
    # Search through all research content
    for layer in research_results:
        for category in layer.get('categories', []):
            for question in category.get('questions', []):
                # Check Layer 3 content
                content = question.get('layer3_content', '')
                if content:
                    for pattern in source_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            sources.add(f"{match} - referenced in AI analysis")
                
                # Check Layer 4 comprehensive content
                if question.get('layer4_comprehensive_report'):
                    comp_content = question['layer4_comprehensive_report'].get('comprehensive_content', '')
                    if comp_content:
                        for pattern in source_patterns:
                            matches = re.findall(pattern, comp_content, re.IGNORECASE)
                            for match in matches:
                                sources.add(f"{match} - referenced in AI analysis")
    
    return list(sources)[:7]  # Return max 7 sources

def get_credible_industry_sources(topic, market):
    """Láº¥y nguá»“n uy tÃ­n theo ngÃ nh vÃ  thá»‹ trÆ°á»ng"""
    topic_lower = topic.lower()
    sources = []
    current_year = datetime.now().year
    
    # Always include these for Vietnam market
    if 'viá»‡t nam' in market.lower() or 'vietnam' in market.lower():
        sources.extend([
            f"Vietnam Economic Times. ({current_year}). Industry Analysis Reports. Retrieved from https://vneconomictimes.com",
            f"Vietnam Investment Review. ({current_year}). Market Intelligence Reports. Retrieved from https://vir.com.vn"
        ])
    
    # Technology/Digital
    if any(keyword in topic_lower for keyword in ['technology', 'digital', 'ai', 'tech', 'cÃ´ng nghá»‡']):
        sources.extend([
            f"Vietnam ICT Statistics. ({current_year}). Ministry of Information and Communications. Retrieved from https://mic.gov.vn",
            f"VINASA Technology Reports. ({current_year}). Vietnam Software Association. Retrieved from https://vinasa.org.vn"
        ])
    
    # Automotive
    elif any(keyword in topic_lower for keyword in ['automotive', 'vehicle', 'Ã´ tÃ´', 'xe']):
        sources.extend([
            f"VAMA Industry Statistics. ({current_year}). Vietnam Automobile Manufacturers Association. Retrieved from https://vama.org.vn",
            f"Vietnam Ministry of Transport. ({current_year}). Transport Statistics. Retrieved from https://mt.gov.vn"
        ])
    
    # Default business sources
    else:
        sources.extend([
            f"Vietnam Business Portal. ({current_year}). Ministry of Planning and Investment. Retrieved from https://business.gov.vn",
            f"VCCI Business Reports. ({current_year}). Vietnam Chamber of Commerce. Retrieved from https://vcci.com.vn"
        ])
    
    return sources[:3]  # Return max 3 additional sources

def generate_ai_executive_summary(data):
    """Generate executive summary using AI based on all research questions and findings"""
    try:
        # Import here to avoid circular import
        from openai_market_research import OpenAIMarketResearch
        
        # Get API key from data or environment
        api_key = data.get('api_key') or os.getenv('OPENAI_API_KEY')
        if not api_key:
            return None
        
        # Initialize AI client
        researcher = OpenAIMarketResearch(
            api_key=api_key,
            industry=data.get('industry', 'Unknown'),
            market=data.get('market', 'Vietnam')
        )
        
        # Collect all main questions from research
        all_questions = []
        research_summary = ""
        
        for layer in data.get('research_results', []):
            layer_name = layer.get('layer_name', '')
            for category in layer.get('categories', []):
                category_name = category.get('category_name', '')
                for question in category.get('questions', []):
                    main_question = question.get('main_question', '')
                    if main_question:
                        all_questions.append(f"â€¢ {main_question}")
                        
                        # Get key insights from Layer 4 or Layer 3
                        layer4_report = question.get('layer4_comprehensive_report', {})
                        if layer4_report:
                            content = layer4_report.get('comprehensive_content', '')
                            # Extract first 2 sentences as key insight
                            sentences = content.split('.')[:2]
                            if sentences:
                                key_insight = '. '.join(sentences).strip()
                                if len(key_insight) > 50:
                                    research_summary += f"\n- {layer_name}/{category_name}: {key_insight[:200]}..."
                        elif question.get('layer3_content'):
                            content = question.get('layer3_content', '')
                            sentences = content.split('.')[:1]
                            if sentences:
                                key_insight = sentences[0].strip()
                                if len(key_insight) > 50:
                                    research_summary += f"\n- {layer_name}/{category_name}: {key_insight[:150]}..."
        
        questions_text = "\n".join(all_questions)
        
        # Create AI prompt for executive summary
        prompt = f"""Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n chiáº¿n lÆ°á»£c, viáº¿t tÃ³m táº¯t Ä‘iá»u hÃ nh (Executive Summary) cho bÃ¡o cÃ¡o nghiÃªn cá»©u thá»‹ trÆ°á»ng.

THÃ”NG TIN NGHIÃŠN Cá»¨U:
- NgÃ nh: {data.get('industry', 'N/A')}
- Thá»‹ trÆ°á»ng: {data.get('market', 'N/A')}
- Má»¥c Ä‘Ã­ch: {data.get('purpose', 'PhÃ¢n tÃ­ch thá»‹ trÆ°á»ng vÃ  cÆ¡ há»™i kinh doanh')}

CÃC CÃ‚U Há»I NGHIÃŠN Cá»¨U ÄÃƒ ÄÆ¯á»¢C PHÃ‚N TÃCH:
{questions_text}

KEY INSIGHTS Tá»ª NGHIÃŠN Cá»¨U:
{research_summary}

VIáº¾T TÃ“M Táº®T ÄIá»€U HÃ€NH theo 5 pháº§n:

**1. ğŸ¯ Má»¤C TIÃŠU/Má»¤C ÄÃCH (80-100 tá»«)**
TÃ³m táº¯t má»¥c tiÃªu nghiÃªn cá»©u vÃ  táº¡i sao quan trá»ng

**2. ğŸŒ PHáº M VI/Bá»I Cáº¢NH (80-100 tá»«)**  
Pháº¡m vi nghiÃªn cá»©u, thá»‹ trÆ°á»ng, phÆ°Æ¡ng phÃ¡p

**3. ğŸ” PHÃT HIá»†N CHÃNH (120-150 tá»«)**
3-4 insight quan trá»ng nháº¥t tá»« nghiÃªn cá»©u

**4. ğŸš€ Äá»€ XUáº¤T HÃ€NH Äá»˜NG (120-150 tá»«)**
5-6 khuyáº¿n nghá»‹ cá»¥ thá»ƒ dá»±a trÃªn findings

**5. ğŸ“ˆ TÃC Äá»˜NG Ká»² Vá»ŒNG (100-120 tá»«)**
Lá»£i Ã­ch vÃ  impact khi Ã¡p dá»¥ng cÃ¡c Ä‘á» xuáº¥t

**YÃŠU Cáº¦U:**
- Viáº¿t chuyÃªn nghiá»‡p, ngáº¯n gá»n, actionable
- Dá»±a trÃªn insights thá»±c tá»« nghiÃªn cá»©u
- TrÃ¡nh chung chung, focus vÃ o specific findings
- Káº¿t thÃºc má»—i pháº§n báº±ng dáº¥u cháº¥m
- Format: Má»—i pháº§n lÃ  má»™t Ä‘oáº¡n vÄƒn liá»n máº¡ch

**CHá»ˆ TRáº¢ Vá»€ Ná»˜I DUNG 5 PHáº¦N, KHÃ”NG CÃ“ GIáº¢I THÃCH HAY INTRO**"""

        # Get AI-generated summary
        ai_summary = researcher.call_openai_api(prompt)
        
        return ai_summary
        
    except Exception as e:
        print(f"âŒ Error generating AI executive summary: {e}")
        return None

def create_executive_summary(doc, data):
    """Táº¡o Executive Summary theo template 5 pháº§n"""
    print("ğŸ“‹ Táº¡o Executive Summary...")
    
    # Get Vietnamese market name for consistency
    vietnamese_market = get_vietnamese_market_name(data.get('market', 'Viá»‡t Nam'))
    
    doc.add_page_break()
    
    # Executive Summary Header
    exec_heading = doc.add_heading('ğŸ“‹ TÃ“M Táº®T ÄIá»€U HÃ€NH (EXECUTIVE SUMMARY)', level=1)
    exec_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(exec_heading, font_size=18)
    
    # 1. Má»¥c tiÃªu / Má»¥c Ä‘Ã­ch
    section1_heading = doc.add_heading('1. ğŸ¯ Má»¤C TIÃŠU / Má»¤C ÄÃCH', level=2)
    set_paragraph_font(section1_heading, font_size=14)
    
    purpose_para = doc.add_paragraph()
    purpose_text = data.get('purpose', '')
    if purpose_text:
        # If custom purpose is provided, format it properly
        formatted_purpose = format_purpose_text(purpose_text)
        if not formatted_purpose.endswith('.'):
            formatted_purpose += '.'
        purpose_para.add_run(f"BÃ¡o cÃ¡o nÃ y nháº±m {formatted_purpose}")
    else:
        purpose_para.add_run("BÃ¡o cÃ¡o nÃ y nháº±m:")
        purpose_para.add_run(f"""
â€¢ Hiá»ƒu thá»‹ trÆ°á»ng tá»•ng thá»ƒ: xu hÆ°á»›ng, quy mÃ´, tá»‘c Ä‘á»™ tÄƒng trÆ°á»Ÿng cá»§a ngÃ nh {data.get('industry', 'N/A')} táº¡i {vietnamese_market}
â€¢ Biáº¿t Ä‘Æ°á»£c mÃ¬nh Ä‘ang Ä‘á»‹nh nháº£y vÃ o thá»‹ trÆ°á»ng lá»›n hay nhá», cháº­t chá»™i hay Ä‘ang má»Ÿ
â€¢ PhÃ¢n tÃ­ch mÃ´i trÆ°á»ng vÄ© mÃ´ áº£nh hÆ°á»Ÿng Ä‘áº¿n ngÃ nh (chÃ­nh trá»‹, kinh táº¿, cÃ´ng nghá»‡...)
â€¢ DÃ¹ng trÆ°á»›c khi quyáº¿t Ä‘á»‹nh cÃ³ nÃªn vÃ o thá»‹ trÆ°á»ng nÃ y khÃ´ng, hoáº·c Ä‘á»ƒ thuyáº¿t phá»¥c nhÃ  Ä‘áº§u tÆ°""")
    
    purpose_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(purpose_para)
    
    # 2. Pháº¡m vi / Bá»‘i cáº£nh
    section2_heading = doc.add_heading('2. ğŸŒ PHáº M VI / Bá»I Cáº¢NH', level=2)
    set_paragraph_font(section2_heading, font_size=14)
    
    scope_para = doc.add_paragraph()
    stats = calculate_statistics(data)
    scope_para.add_run(f"Táº­p trung vÃ o lÄ©nh vá»±c {data.get('industry', 'N/A')} táº¡i thá»‹ trÆ°á»ng {vietnamese_market} trong nÄƒm {datetime.now().year}, sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u phÃ¢n tÃ­ch Ä‘a táº§ng vá»›i {stats['total_questions']} cÃ¢u há»i nghiÃªn cá»©u, Ã¡p dá»¥ng AI Ä‘á»ƒ thu tháº­p vÃ  phÃ¢n tÃ­ch thÃ´ng tin tá»« nhiá»u nguá»“n khÃ¡c nhau.")
    scope_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(scope_para)
    
    # 3. PhÃ¡t hiá»‡n chÃ­nh
    section3_heading = doc.add_heading('3. ğŸ” PHÃT HIá»†N CHÃNH', level=2)
    set_paragraph_font(section3_heading, font_size=14)
    
    # Extract key insights from research content
    insights = extract_key_insights_for_summary(data)
    
    findings_para = doc.add_paragraph()
    findings_para.add_run("ChÃºng tÃ´i nháº­n tháº¥y ráº±ng:")
    for i, insight in enumerate(insights[:4], 1):  # Top 4 insights for findings
        # Capitalize first letter of insight
        insight_text = insight['insight']
        if insight_text and len(insight_text) > 0:
            insight_text = insight_text[0].upper() + insight_text[1:] if len(insight_text) > 1 else insight_text.upper()
        findings_para.add_run(f"\nâ€¢ {insight_text}")
    
    findings_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(findings_para)
    
    # 4. Äá» xuáº¥t hÃ nh Ä‘á»™ng
    section4_heading = doc.add_heading('4. ğŸš€ Äá»€ XUáº¤T HÃ€NH Äá»˜NG', level=2)
    set_paragraph_font(section4_heading, font_size=14)
    
    action_para = doc.add_paragraph()
    action_para.add_run("Dá»±a trÃªn káº¿t quáº£ nghiÃªn cá»©u, chÃºng tÃ´i Ä‘á» xuáº¥t:")
    
    # Generate dynamic recommendations based on research content
    recommendations = generate_recommendations_from_research(data)
    
    if recommendations:
        for rec in recommendations[:5]:  # Top 5 recommendations
            action_para.add_run(f"\nâ€¢ {rec}")
    else:
        # Fallback to generic recommendations
        action_para.add_run(f"""
â€¢ PhÃ¡t triá»ƒn chiáº¿n lÆ°á»£c phÃ¹ há»£p vá»›i xu hÆ°á»›ng thá»‹ trÆ°á»ng Ä‘Ã£ xÃ¡c Ä‘á»‹nh trong nghiÃªn cá»©u
â€¢ Táº­p trung vÃ o cÃ¡c cÆ¡ há»™i Ä‘Æ°á»£c nháº­n diá»‡n qua phÃ¢n tÃ­ch mÃ´i trÆ°á»ng kinh doanh
â€¢ Äá»‘i phÃ³ vá»›i nhá»¯ng thÃ¡ch thá»©c chÃ­nh Ä‘Æ°á»£c chá»‰ ra trong bÃ¡o cÃ¡o
â€¢ XÃ¢y dá»±ng nÄƒng lá»±c cáº¡nh tranh dá»±a trÃªn insights tá»« phÃ¢n tÃ­ch ngÃ nh
â€¢ Thiáº¿t láº­p há»‡ thá»‘ng giÃ¡m sÃ¡t Ä‘á»ƒ theo dÃµi sá»± thay Ä‘á»•i cá»§a cÃ¡c yáº¿u tá»‘ Ä‘Æ°á»£c nghiÃªn cá»©u""")
    
    action_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(action_para)
    
    # 5. TÃ¡c Ä‘á»™ng ká»³ vá»ng
    section5_heading = doc.add_heading('5. ğŸ“ˆ TÃC Äá»˜NG Ká»² Vá»ŒNG', level=2)
    set_paragraph_font(section5_heading, font_size=14)
    
    impact_para = doc.add_paragraph()
    
    # Generate dynamic impact based on research insights
    impact_insights = extract_impact_insights(data)
    
    if impact_insights:
        impact_para.add_run("Viá»‡c thá»±c hiá»‡n cÃ¡c Ä‘á» xuáº¥t trÃªn dá»± kiáº¿n sáº½ mang láº¡i:")
        for impact in impact_insights[:3]:  # Top 3 impacts
            impact_para.add_run(f"\nâ€¢ {impact}")
        
        impact_para.add_run(f"\n\nTá»•ng thá»ƒ, Ä‘iá»u nÃ y sáº½ giÃºp doanh nghiá»‡p nÃ¢ng cao kháº£ nÄƒng cáº¡nh tranh trong ngÃ nh {data.get('industry', 'N/A')} vÃ  thÃ­ch á»©ng tá»‘t hÆ¡n vá»›i mÃ´i trÆ°á»ng kinh doanh nÄƒng Ä‘á»™ng táº¡i {vietnamese_market}.")
    else:
        # Fallback to more generic but still dynamic text
        impact_para.add_run(f"Viá»‡c Ã¡p dá»¥ng cÃ¡c insights tá»« nghiÃªn cá»©u nÃ y sáº½ giÃºp doanh nghiá»‡p nÃ¢ng cao vá»‹ tháº¿ cáº¡nh tranh trong ngÃ nh {data.get('industry', 'N/A')}, tÄƒng cÆ°á»ng kháº£ nÄƒng thÃ­ch á»©ng vá»›i thay Ä‘á»•i thá»‹ trÆ°á»ng táº¡i {vietnamese_market}, vÃ  tá»‘i Æ°u hÃ³a hiá»‡u quáº£ kinh doanh. Dá»± kiáº¿n sáº½ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng ra quyáº¿t Ä‘á»‹nh chiáº¿n lÆ°á»£c vÃ  táº¡o ra lá»£i tháº¿ cáº¡nh tranh bá»n vá»¯ng.")
    
    impact_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(impact_para)
    
    # Footer info
    doc.add_paragraph()
    footer_para = doc.add_paragraph()
    footer_para.add_run('ğŸ“… BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng bá»Ÿi Market Research Automation System').italic = True
    footer_para.add_run(f'\nâ° NgÃ y táº¡o: {datetime.now().strftime("%d/%m/%Y %H:%M")}')
    footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(footer_para, font_size=9)

def extract_key_insights_for_summary(data):
    """TrÃ­ch xuáº¥t key insights Ä‘á»ƒ táº¡o executive summary pháº§n phÃ¡t hiá»‡n chÃ­nh"""
    insights = []
    
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            
            for question in category.get('questions', []):
                # Láº¥y tá»« comprehensive report trÆ°á»›c
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    content = layer4_comprehensive.get('comprehensive_content', '')
                    if content:
                        # Extract key trends and opportunities
                        sentences = content.split('.')
                        for sentence in sentences:
                            if any(keyword in sentence.lower() for keyword in ['xu hÆ°á»›ng', 'cÆ¡ há»™i', 'thÃ¡ch thá»©c', 'tÄƒng trÆ°á»Ÿng', 'phÃ¡t triá»ƒn']):
                                insight = sentence.strip()
                                if len(insight) > 50:  # Only meaningful insights
                                    insights.append({
                                        'category': f"{layer_name} - {category_name}",
                                        'insight': insight[:150] + "..." if len(insight) > 150 else insight
                                    })
                                    break
                # Fallback to layer 3
                elif question.get('layer3_content'):
                    content = question.get('layer3_content', '')
                    sentences = content.split('.')
                    for sentence in sentences:
                        if any(keyword in sentence.lower() for keyword in ['quan trá»ng', 'chÃ­nh', 'Ä‘Ã¡ng chÃº Ã½', 'ná»•i báº­t']):
                            insight = sentence.strip()
                            if len(insight) > 50:
                                insights.append({
                                    'category': f"{layer_name} - {category_name}",
                                    'insight': insight[:120] + "..." if len(insight) > 120 else insight
                                })
                                break
    
    return insights[:6]  # Top 6 insights for summary

def generate_recommendations_from_research(data):
    """Generate dynamic recommendations based on research content"""
    recommendations = []
    
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            
            for question in category.get('questions', []):
                # Láº¥y tá»« comprehensive report trÆ°á»›c
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    content = layer4_comprehensive.get('comprehensive_content', '')
                    if content:
                        # Extract actionable recommendations
                        sentences = content.split('.')
                        for sentence in sentences:
                            # Look for actionable insights and recommendations
                            if any(keyword in sentence.lower() for keyword in [
                                'nÃªn', 'cáº§n', 'khuyáº¿n nghá»‹', 'Ä‘á» xuáº¥t', 'tÄƒng cÆ°á»ng', 
                                'phÃ¡t triá»ƒn', 'Ä‘áº§u tÆ°', 'táº­p trung', 'xÃ¢y dá»±ng', 'thÃºc Ä‘áº©y'
                            ]):
                                rec = sentence.strip()
                                if len(rec) > 30 and rec not in recommendations:  # Avoid duplicates
                                    recommendations.append(rec[:200] + "..." if len(rec) > 200 else rec)
                                    if len(recommendations) >= 8:  # Enough recommendations
                                        break
                
                # Fallback to layer 3 content
                elif question.get('layer3_content'):
                    content = question.get('layer3_content', '')
                    sentences = content.split('.')
                    for sentence in sentences:
                        if any(keyword in sentence.lower() for keyword in [
                            'nÃªn', 'cáº§n', 'quan trá»ng', 'chÃ­nh', 'Æ°u tiÃªn'
                        ]):
                            rec = sentence.strip()
                            if len(rec) > 30 and rec not in recommendations:
                                recommendations.append(rec[:150] + "..." if len(rec) > 150 else rec)
                                if len(recommendations) >= 8:
                                    break
    
    return recommendations[:6]  # Top 6 actionable recommendations

def extract_impact_insights(data):
    """Extract expected impact insights from research content"""
    impacts = []
    
    for layer in data.get('research_results', []):
        for category in layer.get('categories', []):
            for question in category.get('questions', []):
                # Check comprehensive reports first
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    content = layer4_comprehensive.get('comprehensive_content', '')
                    if content:
                        sentences = content.split('.')
                        for sentence in sentences:
                            # Look for impact-related statements
                            if any(keyword in sentence.lower() for keyword in [
                                'tÃ¡c Ä‘á»™ng', 'áº£nh hÆ°á»Ÿng', 'hiá»‡u quáº£', 'káº¿t quáº£', 'lá»£i Ã­ch',
                                'cáº£i thiá»‡n', 'tÄƒng trÆ°á»Ÿng', 'giáº£m thiá»ƒu', 'tá»‘i Æ°u', 'nÃ¢ng cao'
                            ]):
                                impact = sentence.strip()
                                if len(impact) > 40 and impact not in impacts:
                                    impacts.append(impact[:180] + "..." if len(impact) > 180 else impact)
                                    if len(impacts) >= 5:
                                        break
                
                # Fallback to layer 3
                elif question.get('layer3_content'):
                    content = question.get('layer3_content', '')
                    sentences = content.split('.')
                    for sentence in sentences:
                        if any(keyword in sentence.lower() for keyword in [
                            'lá»£i Ã­ch', 'hiá»‡u quáº£', 'cáº£i thiá»‡n', 'tÄƒng', 'giáº£m'
                        ]):
                            impact = sentence.strip()
                            if len(impact) > 40 and impact not in impacts:
                                impacts.append(impact[:150] + "..." if len(impact) > 150 else impact)
                                if len(impacts) >= 5:
                                    break
    
    return impacts[:4]  # Top 4 impact insights

def clean_comprehensive_content(content):
    """Remove numbering, section headers, and intro sentences from comprehensive content"""
    if not content:
        return content
    
    # Remove intro sentences like "Äá»ƒ tráº£ lá»i chuyÃªn sÃ¢u cÃ¢u há»i vá»..."
    cleaned_content = re.sub(r'^Äá»ƒ tráº£ lá»i[^,]*,\s*', '', content, flags=re.MULTILINE)
    cleaned_content = re.sub(r'^Äá»ƒ tráº£ lá»i[^.]*\.\s*', '', cleaned_content, flags=re.MULTILINE)
    cleaned_content = re.sub(r'^Nháº±m tráº£ lá»i[^,]*,\s*', '', cleaned_content, flags=re.MULTILINE)
    cleaned_content = re.sub(r'^Nháº±m phÃ¢n tÃ­ch[^,]*,\s*', '', cleaned_content, flags=re.MULTILINE)
    
    # Remove section headers like "ğŸ“Š PHÃ‚N TÃCH HIá»†N TRáº NG:", "âš¡ DRIVERS & IMPACTS:", etc.
    cleaned_content = re.sub(r'\n*[ğŸ“Šâš¡ğŸ“ˆâš ï¸ğŸš€]\s*[A-ZÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÄÃ‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃÃŒá»ˆÄ¨á»ŠÃ“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´\s&]+:\s*', '\n\n', cleaned_content)
    
    # Remove numbered sections like "1. SECTION:", "2. ANALYSIS:", etc.
    cleaned_content = re.sub(r'\n*\d+\.\s+[A-ZÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÄÃ‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃÃŒá»ˆÄ¨á»ŠÃ“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´\s]+:\s*', '\n\n', cleaned_content)
    
    # Remove **bold headers** at start of lines
    cleaned_content = re.sub(r'\n\*\*[^*]+\*\*\s*\(\d+-\d+\s+tá»«\)\s*\n', '\n\n', cleaned_content)
    
    # Remove intro phrases at the beginning
    intro_patterns = [
        r'^Trong bá»‘i cáº£nh nÃ y,\s*',
        r'^ChÃºng ta cáº§n xem xÃ©t[^.]*\.\s*',
        r'^Viá»‡c phÃ¢n tÃ­ch[^.]*\.\s*'
    ]
    
    for pattern in intro_patterns:
        cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.MULTILINE)
    
    # Clean up extra newlines
    cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)
    
    # Remove leading/trailing whitespace
    cleaned_content = cleaned_content.strip()
    
    return cleaned_content

def create_comprehensive_word_report(json_file: str, output_file: str = None, use_vietnamese_filename: bool = True) -> str:
    """
    Táº¡o bÃ¡o cÃ¡o Word toÃ n diá»‡n tá»« káº¿t quáº£ nghiÃªn cá»©u JSON
    
    Args:
        json_file: ÄÆ°á»ng dáº«n file JSON
        output_file: ÄÆ°á»ng dáº«n file output (optional)
        use_vietnamese_filename: Sá»­ dá»¥ng tÃªn file tiáº¿ng Viá»‡t thÃ¢n thiá»‡n hay technical name
    """
    
    print("ğŸ“„ Táº¡o trang bÃ¬a...")
    # Äá»c dá»¯ liá»‡u
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if not output_file:
        # Extract base info for filename
        base_name = os.path.basename(json_file).replace('layer3_research_', '').replace('.json', '')
        
        if use_vietnamese_filename:
            # Táº¡o tÃªn file tiáº¿ng Viá»‡t thÃ¢n thiá»‡n
            metadata = data.get('research_metadata', {})
            industry = metadata.get('industry', 'Unknown')
            market = metadata.get('market', 'Vietnam')
            timestamp = metadata.get('research_timestamp', '').replace(':', '').replace(' ', '_').replace('-', '')
            
            # Clean industry name for filename
            industry_clean = re.sub(r'[^\w\s-]', '', industry)
            industry_clean = re.sub(r'\s+', '_', industry_clean)
            
            # Create friendly Vietnamese filename
            output_file = f"BÃ¡o_cÃ¡o_nghiÃªn_cá»©u_thá»‹_trÆ°á»ng_{industry_clean}_{timestamp[:8]}.docx"
        else:
            # Use technical naming convention
            output_file = f"{base_name}_comprehensive_report.docx"
        
        # Ensure output directory
        output_dir = os.path.dirname(json_file) if '/' in json_file else 'output'
        output_file = os.path.join(output_dir, output_file)
    
    # Create document
    doc = Document()
    add_custom_styles(doc)
    
    # ===== TRANG BÃŒA =====
    print("ğŸ“„ Táº¡o trang bÃ¬a...")
    
    title = doc.add_heading('BÃO CÃO NGHIÃŠN Cá»¨U THá»Š TRÆ¯á»œNG', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(title, font_size=18)
    
    doc.add_paragraph()
    
    # ThÃ´ng tin cÆ¡ báº£n vá»›i Table
    info_heading = doc.add_heading('ğŸ“Š THÃ”NG TIN Tá»”NG QUAN', level=2)
    info_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(info_heading, font_size=14)
    
    create_info_table(doc, data)
    
    doc.add_paragraph()
    
    # Purpose
    if data.get('purpose'):
        purpose_heading = doc.add_heading('ğŸ¯ Má»¤C ÄÃCH NGHIÃŠN Cá»¨U', level=2)
        purpose_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_paragraph_font(purpose_heading, font_size=14)
        
        purpose_para = doc.add_paragraph(data['purpose'])
        purpose_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        set_paragraph_font(purpose_para)
    
    doc.add_page_break()
    
    # ===== Ná»˜I DUNG CHÃNH =====
    print("ğŸ” Táº¡o ná»™i dung bÃ¡o cÃ¡o chi tiáº¿t...")
    
    results_heading = doc.add_heading('ğŸ“‹ Káº¾T QUáº¢ NGHIÃŠN Cá»¨U CHI TIáº¾T', level=1)
    results_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(results_heading, font_size=16)
    
    for layer_idx, layer in enumerate(data.get('research_results', []), 1):
        layer_name = layer.get('layer_name', '')
        
        print(f"  ğŸ”¥ Xá»­ lÃ½ Layer: {layer_name}")
        
        # Layer heading
        layer_heading = doc.add_heading(f"{layer_idx}. {layer_name.upper()}", level=1)
        set_paragraph_font(layer_heading, font_size=16)
        
        for cat_idx, category in enumerate(layer.get('categories', []), 1):
            category_name = category.get('category_name', '')
            
            print(f"    ğŸ“‹ Category: {category_name}")
            
            # Category heading
            category_heading = doc.add_heading(f"{cat_idx}. {category_name}", level=2)
            set_paragraph_font(category_heading, font_size=14)
            
            # Check if this is Layer 3 comprehensive category analysis
            layer3_comprehensive_category = category.get('layer3_comprehensive_category', {})
            if layer3_comprehensive_category:
                print(f"    ğŸ¯ Layer 3 Comprehensive Category Analysis: {category_name}")
                
                # Display Layer 3 comprehensive analysis for entire category
                comprehensive_content = layer3_comprehensive_category.get('comprehensive_content', '')
                if comprehensive_content:
                    # Clean up content
                    cleaned_content = clean_comprehensive_content(comprehensive_content).strip()
                    
                    # Display analysis content directly without headers
                    comp_para = doc.add_paragraph(cleaned_content)
                    comp_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                    set_paragraph_font(comp_para)
                
                # Timestamp only
                timestamp = layer3_comprehensive_category.get('analysis_timestamp', '')
                if timestamp:
                    time_para = doc.add_paragraph()
                    time_para.add_run(f"â° PhÃ¢n tÃ­ch: {timestamp}").italic = True
                    time_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
                    set_paragraph_font(time_para, font_size=9)
                
                # Skip individual question processing for Layer 3 mode
                continue
            
            # Layer 4 mode: Process individual questions (existing logic)
            for q_idx, question in enumerate(category.get('questions', []), 1):
                main_question = question.get('main_question', '')
                
                print(f"      â“ Question: {main_question[:50]}...")
                
                # Main question heading
                question_heading = doc.add_heading(f"{q_idx}. {main_question}", level=3)
                set_paragraph_font(question_heading, font_size=12)
                
                # Layer 3 content (more concise)
                layer3_content = question.get('layer3_content', '')
                if layer3_content:
                    # Clean up and make more concise
                    cleaned_content = layer3_content.strip()
                    
                    layer3_para = doc.add_paragraph()
                    layer3_para.add_run('ğŸ“‹ PHÃ‚N TÃCH Tá»”NG QUAN:').bold = True
                    layer3_para.add_run('\n' + cleaned_content)
                    layer3_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                    set_paragraph_font(layer3_para)
                
                # Layer 4 comprehensive report (Æ°u tiÃªn)
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    print(f"        ğŸ” CÃ³ comprehensive report")
                    
                    comprehensive_content = layer4_comprehensive.get('comprehensive_content', '')
                    if comprehensive_content:
                        # Clean up content and make more structured
                        cleaned_comp_content = clean_comprehensive_content(comprehensive_content).strip()
                        
                        comp_para = doc.add_paragraph()
                        comp_para.add_run('ğŸ¯ PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:').bold = True
                        comp_para.add_run('\n' + cleaned_comp_content)
                        comp_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                        set_paragraph_font(comp_para)
                    
                    # Timestamp
                    timestamp = layer4_comprehensive.get('enhancement_timestamp', '')
                    if timestamp:
                        time_para = doc.add_paragraph()
                        time_para.add_run(f"â° Cáº­p nháº­t: {timestamp}").italic = True
                        time_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
                        set_paragraph_font(time_para, font_size=9)
                
                # Layer 4 individual enhancements (fallback)
                elif question.get('layer4_enhancements', {}):
                    layer4_enhancements = question.get('layer4_enhancements', {})
                    print(f"        ğŸ¯ CÃ³ {len(layer4_enhancements)} individual enhancements")
                    
                    layer4_heading = doc.add_paragraph()
                    layer4_heading.add_run('ğŸ¯ PHÃ‚N TÃCH CHI TIáº¾T:').bold = True
                    
                    for sub_question, enhancement_data in layer4_enhancements.items():
                        # Sub-question
                        sub_q_para = doc.add_paragraph()
                        sub_q_para.add_run(f"â€¢ {sub_question}").bold = True
                        set_paragraph_font(sub_q_para)
                        
                        # Enhanced content
                        enhanced_content = enhancement_data.get('enhanced_content', '')
                        if enhanced_content:
                            enh_para = doc.add_paragraph(enhanced_content)
                            enh_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                            enh_para.paragraph_format.left_indent = Inches(0.3)
                            set_paragraph_font(enh_para)
                        
                        # Timestamp
                        timestamp = enhancement_data.get('enhancement_timestamp', '')
                        if timestamp:
                            time_para = doc.add_paragraph()
                            time_para.add_run(f"â° Cáº­p nháº­t: {timestamp}").italic = True
                            time_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
                            set_paragraph_font(time_para, font_size=9)
    
    # ===== REFERENCES SECTION =====
    create_references_section(doc, data)
    
    # ===== EXECUTIVE SUMMARY =====
    print("ğŸ“‹ Generating AI-powered Executive Summary...")
    
    # Try AI-generated summary first
    ai_summary = generate_ai_executive_summary(data)
    
    if ai_summary:
        # Add page break before executive summary
        doc.add_page_break()
        
        # Executive Summary Header
        exec_heading = doc.add_heading('ğŸ“‹ TÃ“M Táº®T ÄIá»€U HÃ€NH (EXECUTIVE SUMMARY)', level=1)
        exec_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_paragraph_font(exec_heading, font_size=18)
        
        # Add AI-generated content
        ai_summary_para = doc.add_paragraph(ai_summary)
        ai_summary_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        set_paragraph_font(ai_summary_para)
        
        # Footer info
        doc.add_paragraph()
        footer_para = doc.add_paragraph()
        footer_para.add_run('ğŸ“… BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng bá»Ÿi Market Research Automation System').italic = True
        footer_para.add_run(f'\nâ° NgÃ y táº¡o: {datetime.now().strftime("%d/%m/%Y %H:%M")}')
        footer_para.add_run(f'\nğŸ¤– Executive Summary generated by AI based on research findings')
        footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_paragraph_font(footer_para, font_size=9)
    else:
        # Fallback to original executive summary
        print("âš ï¸ AI summary failed, using fallback...")
        create_executive_summary(doc, data)
    
    # Save document
    print(f"ğŸ’¾ LÆ°u file: {output_file}")
    doc.save(output_file)
    print(f"âœ… ÄÃ£ táº¡o Word document: {output_file}")
    
    return output_file

def calculate_statistics(data):
    """TÃ­nh toÃ¡n thá»‘ng kÃª cho bÃ¡o cÃ¡o"""
    stats = {
        'total_layers': 0,
        'total_categories': 0,
        'total_questions': 0,
        'comprehensive_reports': 0,
        'individual_enhancements': 0
    }
    
    research_results = data.get('research_results', [])
    stats['total_layers'] = len(research_results)
    
    for layer in research_results:
        categories = layer.get('categories', [])
        stats['total_categories'] += len(categories)
        
        for category in categories:
            questions = category.get('questions', [])
            stats['total_questions'] += len(questions)
            
            for question in questions:
                # Count comprehensive reports
                if question.get('layer4_comprehensive_report'):
                    stats['comprehensive_reports'] += 1
                
                # Count individual enhancements
                individual_enhancements = question.get('layer4_enhancements', {})
                stats['individual_enhancements'] += len(individual_enhancements)
    
    return stats

def find_latest_research_file():
    """TÃ¬m file research má»›i nháº¥t trong thÆ° má»¥c output"""
    output_dir = 'output'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    files = []
    for file in os.listdir(output_dir):
        if file.startswith('layer3_research_') and file.endswith('.json'):
            files.append(os.path.join(output_dir, file))
    
    if not files:
        # Fallback: tÃ¬m trong thÆ° má»¥c gá»‘c
        files = [f for f in os.listdir('.') if f.startswith('layer3_research_') and f.endswith('.json')]
        
    if not files:
        return None
    
    # Sáº¯p xáº¿p theo thá»i gian modification
    files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    return files[0]

def main():
    """Main function"""
    print("ğŸ”¬ COMPREHENSIVE WORD EXPORT TOOL")
    print("="*50)
    
    # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
    output_dir = 'output'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ ÄÃ£ táº¡o thÆ° má»¥c: {output_dir}")
    
    # TÃ¬m file research
    json_file = find_latest_research_file()
    
    if not json_file:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file research results!")
        print("ğŸ’¡ HÃ£y cháº¡y Layer 3 research trÆ°á»›c.")
        return
    
    print(f"ğŸ“ Sá»­ dá»¥ng file: {json_file}")
    
    # Táº¡o output filename trong thÆ° má»¥c output
    base_name = os.path.basename(json_file).replace('layer3_research_', '').replace('.json', '')
    output_file = os.path.join(output_dir, f"BÃ¡o_cÃ¡o_nghiÃªn_cá»©u_thá»‹_trÆ°á»ng_{base_name}.docx")
    
    # Export
    result = create_comprehensive_word_report(json_file, output_file)
    
    if result:
        print(f"\nğŸ‰ HOÃ€N THÃ€NH!")
        print(f"ğŸ“„ File Word: {result}")
        print(f"ğŸ’¡ CÃ³ thá»ƒ má»Ÿ file báº±ng: open \"{result}\"")
    else:
        print("âŒ Export tháº¥t báº¡i!")

def format_purpose_text(purpose_text):
    """Format purpose text to ensure proper capitalization for both bullet and dash formats"""
    if not purpose_text:
        return purpose_text
    
    # Split by common delimiters and capitalize each item
    lines = purpose_text.split('\n')
    formatted_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Handle both bullet (â€¢) and dash (-) formats
        if line.startswith('â€¢') or line.startswith('-'):
            # Extract the text after the bullet/dash
            prefix = line[0]  # â€¢ or -
            text = line[1:].strip()
            if text:
                # Capitalize first letter
                text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()
                formatted_lines.append(f"{prefix} {text}")
        else:
            # Regular text - capitalize first letter
            if line:
                line = line[0].upper() + line[1:] if len(line) > 1 else line.upper()
                formatted_lines.append(line)
    
    return '\n'.join(formatted_lines)

if __name__ == "__main__":
    main() 