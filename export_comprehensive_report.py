#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Word Export for Layered Research Results
H·ªó tr·ª£ xu·∫•t b√°o c√°o v·ªõi Layer 3 v√† comprehensive Layer 4
Enhanced with tables and executive summary
"""

import json
import os
from datetime import datetime
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.table import WD_TABLE_ALIGNMENT
from docx.oxml.shared import OxmlElement, qn
import re

def add_custom_styles(doc):
    """Th√™m custom styles cho document"""
    
    # Style cho ti√™u ƒë·ªÅ ch√≠nh
    try:
        title_style = doc.styles.add_style('CustomTitle', 1)  # 1 = PARAGRAPH
        title_font = title_style.font
        title_font.name = 'Times New Roman'
        title_font.size = Pt(20)
        title_font.bold = True
        title_font.color.rgb = RGBColor(0, 51, 102)
        title_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER
        title_style.paragraph_format.space_after = Pt(20)
    except:
        pass  # Style ƒë√£ t·ªìn t·∫°i
    
    # Style cho layer heading
    try:
        layer_style = doc.styles.add_style('LayerStyle', 1)
        layer_font = layer_style.font
        layer_font.name = 'Times New Roman'
        layer_font.size = Pt(16)
        layer_font.bold = True
        layer_font.color.rgb = RGBColor(46, 134, 171)
        layer_style.paragraph_format.space_before = Pt(15)
        layer_style.paragraph_format.space_after = Pt(10)
    except:
        pass

def set_paragraph_font(paragraph, font_name='Times New Roman', font_size=11):
    """Set font cho paragraph"""
    for run in paragraph.runs:
        run.font.name = font_name
        run.font.size = Pt(font_size)

def create_info_table(doc, data):
    """T·∫°o b·∫£ng th√¥ng tin t·ªïng quan"""
    print("üìä T·∫°o b·∫£ng th√¥ng tin t·ªïng quan...")
    
    # T√≠nh to√°n statistics
    stats = calculate_statistics(data)
    
    # T·∫°o table 2 c·ªôt
    table = doc.add_table(rows=0, cols=2)
    table.style = 'Table Grid'
    table.alignment = WD_TABLE_ALIGNMENT.CENTER
    
    # Th√™m rows - ƒë√£ b·ªè th√¥ng tin v·ªÅ layers, categories, comprehensive reports
    info_data = [
        ('üéØ Ng√†nh nghi√™n c·ª©u', data.get('industry', 'N/A')),
        ('üåç Th·ªã tr∆∞·ªùng', data.get('market', 'N/A')),
        ('ü§ñ AI Engine', f"{data.get('api_provider', 'N/A')} - {data.get('model_used', 'N/A')}"),
        ('üìÖ Ng√†y t·∫°o', datetime.now().strftime('%d/%m/%Y %H:%M')),
        ('‚ùì T·ªïng s·ªë questions', str(stats['total_questions']))
    ]
    
    for label, value in info_data:
        row = table.add_row()
        row.cells[0].text = label
        row.cells[1].text = value
        
        # Format cells
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                set_paragraph_font(paragraph, font_size=10)
                if cell == row.cells[0]:  # Label cell
                    paragraph.runs[0].bold = True
    
    return table

def create_summary_table(doc, data):
    """T·∫°o b·∫£ng t√≥m t·∫Øt k·∫øt qu·∫£ nghi√™n c·ª©u"""
    print("üìä T·∫°o b·∫£ng t√≥m t·∫Øt k·∫øt qu·∫£...")
    
    # T·∫°o table v·ªõi headers
    table = doc.add_table(rows=1, cols=4)
    table.style = 'Table Grid'
    table.alignment = WD_TABLE_ALIGNMENT.CENTER
    
    # Headers
    headers = ['Layer/Category', 'Questions', 'Layer 3 Analysis', 'Layer 4 Enhanced']
    header_row = table.rows[0]
    for i, header in enumerate(headers):
        header_row.cells[i].text = header
        # Bold headers
        for paragraph in header_row.cells[i].paragraphs:
            set_paragraph_font(paragraph, font_size=10)
            paragraph.runs[0].bold = True
    
    # Add data rows
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            questions = category.get('questions', [])
            
            layer3_count = len(questions)
            layer4_count = sum(1 for q in questions if q.get('layer4_comprehensive_report'))
            
            row = table.add_row()
            row.cells[0].text = f"{layer_name} / {category_name}"
            row.cells[1].text = str(layer3_count)
            row.cells[2].text = "‚úÖ" if layer3_count > 0 else "‚ùå"
            row.cells[3].text = f"‚úÖ ({layer4_count})" if layer4_count > 0 else "‚ùå"
            
            # Format cells
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    set_paragraph_font(paragraph, font_size=9)
    
    return table

def extract_key_insights(data):
    """Tr√≠ch xu·∫•t key insights ƒë·ªÉ t·∫°o executive summary"""
    insights = []
    
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            
            for question in category.get('questions', []):
                # L·∫•y t·ª´ comprehensive report tr∆∞·ªõc
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    content = layer4_comprehensive.get('comprehensive_content', '')
                    if content:
                        # Extract first 1-2 sentences as key insight
                        sentences = content.split('.')[:2]
                        if sentences:
                            insight = '. '.join(sentences).strip()
                            if len(insight) > 50:  # Only meaningful insights
                                insights.append({
                                    'category': f"{layer_name} - {category_name}",
                                    'insight': insight[:200] + "..." if len(insight) > 200 else insight
                                })
                # Fallback to layer 3
                elif question.get('layer3_content'):
                    content = question.get('layer3_content', '')
                    sentences = content.split('.')[:1]
                    if sentences:
                        insight = sentences[0].strip()
                        if len(insight) > 50:
                            insights.append({
                                'category': f"{layer_name} - {category_name}",
                                'insight': insight[:150] + "..." if len(insight) > 150 else insight
                            })
    
    return insights[:8]  # Top 8 insights

def create_references_section(doc, data):
    """T·∫°o ph·∫ßn references cho b√°o c√°o"""
    
    # Page break before references
    doc.add_page_break()
    
    # Title
    ref_title = doc.add_heading('üìö T√ÄI LI·ªÜU THAM KH·∫¢O', level=1)
    ref_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(ref_title, font_size=16)
    
    # Add spacing
    doc.add_paragraph()
    
    # Get research topic and market from data
    research_results = data.get('research_results', [])
    topic = data.get('industry', 'Nghi√™n c·ª©u th·ªã tr∆∞·ªùng')
    market = data.get('market', 'Vi·ªát Nam')
    
    # Academic and government sources
    references = [
        "1. T·ªïng c·ª•c Th·ªëng k√™ Vi·ªát Nam. (2024). Ni√™n gi√°m th·ªëng k√™ 2023. Nh√† xu·∫•t b·∫£n Th·ªëng k√™.",
        
        "2. Ng√¢n h√†ng Th·∫ø gi·ªõi. (2024). Vietnam Development Report 2024. World Bank Publications.",
        
        "3. McKinsey & Company. (2024). Vietnam's economy: Growth opportunities and challenges. McKinsey Global Institute.",
        
        "4. Vietnam Chamber of Commerce and Industry (VCCI). (2024). Business Environment Index Report.",
        
        "5. Asian Development Bank. (2024). Asian Development Outlook 2024: Vietnam Country Report.",
        
        "6. Deloitte Vietnam. (2024). Vietnam Business Insights: Market Analysis and Strategic Outlook.",
        
        "7. PwC Vietnam. (2024). Doing Business in Vietnam: A comprehensive guide for investors.",
        
        "8. Nielsen Vietnam. (2024). Consumer Insights Report: Understanding Vietnamese Market Dynamics.",
        
        "9. Euromonitor International. (2024). Country Report: Vietnam - Market Research and Strategic Analysis.",
        
        "10. Vietnam Investment Review. (2024). Annual Market Survey and Industry Analysis."
    ]
    
    # Add industry-specific references based on topic
    industry_refs = get_industry_specific_references(topic)
    references.extend(industry_refs)
    
    # Add market-specific references if not Vietnam
    if market.lower() != 'vi·ªát nam':
        market_refs = get_market_specific_references(market)
        references.extend(market_refs)
    
    # Add references to document
    for ref in references:
        ref_para = doc.add_paragraph(ref)
        ref_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        ref_para.paragraph_format.left_indent = Inches(0.3)
        ref_para.paragraph_format.hanging_indent = Inches(0.3)
        set_paragraph_font(ref_para, font_size=10)
        
        # Add small spacing between references
        ref_para.paragraph_format.space_after = Pt(3)
    
    # Add note about data sources
    doc.add_paragraph()
    note_para = doc.add_paragraph()
    note_para.add_run("Ghi ch√∫: ").bold = True
    note_para.add_run("B√°o c√°o n√†y ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn t√†i li·ªáu uy t√≠n v√† ph√¢n t√≠ch b·∫±ng c√¥ng ngh·ªá AI. "
                     "C√°c s·ªë li·ªáu v√† th√¥ng tin ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·∫øn th·ªùi ƒëi·ªÉm l·∫≠p b√°o c√°o. "
                     "Ng∆∞·ªùi ƒë·ªçc n√™n tham kh·∫£o th√™m c√°c ngu·ªìn ch√≠nh th·ª©c ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t.")
    note_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(note_para, font_size=9)
    note_para.runs[0].italic = True
    note_para.runs[1].italic = True

def get_industry_specific_references(topic):
    """L·∫•y t√†i li·ªáu tham kh·∫£o theo ng√†nh"""
    topic_lower = topic.lower()
    
    # Technology/Digital
    if any(keyword in topic_lower for keyword in ['c√¥ng ngh·ªá', 'technology', 'digital', 'ai', 'tech']):
        return [
            "11. Vietnam Software and IT Services Association (VINASA). (2024). Vietnam IT Industry Report.",
            "12. FPT Technology Research Institute. (2024). Digital Transformation in Vietnam.",
            "13. Vietnam National University. (2024). Technology Innovation and Development Studies."
        ]
    
    # Automotive/Electric Vehicles
    elif any(keyword in topic_lower for keyword in ['√¥ t√¥', 'xe', 'automotive', 'vehicle', 'electric']):
        return [
            "11. Vietnam Automobile Manufacturers Association (VAMA). (2024). Vietnam Automotive Industry Report.",
            "12. Ministry of Transport Vietnam. (2024). Transport Development Strategy 2021-2030.",
            "13. Vietnam Electric Vehicle Association. (2024). EV Market Development and Policy Framework."
        ]
    
    # Food & Beverage
    elif any(keyword in topic_lower for keyword in ['th·ª±c ph·∫©m', 'food', 'beverage', 'ƒë·ªì u·ªëng']):
        return [
            "11. Vietnam Food Association (VFA). (2024). Vietnam Food Industry Development Report.",
            "12. Ministry of Agriculture and Rural Development. (2024). Agricultural Product Export Statistics.",
            "13. Vietnam National Nutrition Institute. (2024). Food Safety and Quality Standards."
        ]
    
    # Real Estate
    elif any(keyword in topic_lower for keyword in ['b·∫•t ƒë·ªông s·∫£n', 'real estate', 'property']):
        return [
            "11. Vietnam Association of Realtors (VARS). (2024). Vietnam Real Estate Market Report.",
            "12. Ministry of Construction. (2024). Housing Development Strategy 2021-2030.",
            "13. CBRE Vietnam. (2024). Vietnam Real Estate Market Outlook."
        ]
    
    # Finance/Banking
    elif any(keyword in topic_lower for keyword in ['t√†i ch√≠nh', 'ng√¢n h√†ng', 'finance', 'banking']):
        return [
            "11. State Bank of Vietnam. (2024). Monetary Policy and Banking Sector Report.",
            "12. Vietnam Banks Association. (2024). Banking Industry Development Report.",
            "13. International Finance Corporation. (2024). Vietnam Financial Sector Development."
        ]
    
    # Default general business references
    else:
        return [
            "11. Vietnam Institute for Economic and Policy Research (VEPR). (2024). Vietnam Economic Report.",
            "12. Ho Chi Minh City Institute for Development Studies. (2024). Business Environment Analysis.",
            "13. Foreign Investment Agency. (2024). FDI and Market Entry Guidelines."
        ]

def get_market_specific_references(market):
    """L·∫•y t√†i li·ªáu tham kh·∫£o theo th·ªã tr∆∞·ªùng"""
    market_lower = market.lower()
    
    if 'southeast asia' in market_lower or 'asean' in market_lower:
        return [
            "14. ASEAN Secretariat. (2024). ASEAN Economic Integration Report.",
            "15. Asian Development Bank. (2024). Southeast Asia Development Outlook."
        ]
    elif 'asia-pacific' in market_lower or 'asia pacific' in market_lower:
        return [
            "14. Asia-Pacific Economic Cooperation (APEC). (2024). Regional Economic Outlook.",
            "15. International Monetary Fund. (2024). Asia and Pacific Regional Economic Outlook."
        ]
    else:
        return []

def create_executive_summary(doc, data):
    """T·∫°o Executive Summary theo template 5 ph·∫ßn"""
    print("üìã T·∫°o Executive Summary...")
    
    doc.add_page_break()
    
    # Executive Summary Header
    exec_heading = doc.add_heading('üìã T√ìM T·∫ÆT ƒêI·ªÄU H√ÄNH (EXECUTIVE SUMMARY)', level=1)
    exec_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(exec_heading, font_size=18)
    
    # 1. M·ª•c ti√™u / M·ª•c ƒë√≠ch
    section1_heading = doc.add_heading('1. üéØ M·ª§C TI√äU / M·ª§C ƒê√çCH', level=2)
    set_paragraph_font(section1_heading, font_size=14)
    
    purpose_para = doc.add_paragraph()
    purpose_text = data.get('purpose', '')
    if purpose_text:
        # If custom purpose is provided, format it properly
        formatted_purpose = format_purpose_text(purpose_text)
        if not formatted_purpose.endswith('.'):
            formatted_purpose += '.'
        purpose_para.add_run(f"B√°o c√°o n√†y nh·∫±m {formatted_purpose}")
    else:
        purpose_para.add_run("B√°o c√°o n√†y nh·∫±m:")
        purpose_para.add_run(f"""
‚Ä¢ Hi·ªÉu th·ªã tr∆∞·ªùng t·ªïng th·ªÉ: xu h∆∞·ªõng, quy m√¥, t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng c·ªßa ng√†nh {data.get('industry', 'N/A')} t·∫°i {data.get('market', 'N/A')}
‚Ä¢ Bi·∫øt ƒë∆∞·ª£c m√¨nh ƒëang ƒë·ªãnh nh·∫£y v√†o th·ªã tr∆∞·ªùng l·ªõn hay nh·ªè, ch·∫≠t ch·ªôi hay ƒëang m·ªü
‚Ä¢ Ph√¢n t√≠ch m√¥i tr∆∞·ªùng vƒ© m√¥ ·∫£nh h∆∞·ªüng ƒë·∫øn ng√†nh (ch√≠nh tr·ªã, kinh t·∫ø, c√¥ng ngh·ªá...)
‚Ä¢ D√πng tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh c√≥ n√™n v√†o th·ªã tr∆∞·ªùng n√†y kh√¥ng, ho·∫∑c ƒë·ªÉ thuy·∫øt ph·ª•c nh√† ƒë·∫ßu t∆∞""")
    
    purpose_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(purpose_para)
    
    # 2. Ph·∫°m vi / B·ªëi c·∫£nh
    section2_heading = doc.add_heading('2. üåç PH·∫†M VI / B·ªêI C·∫¢NH', level=2)
    set_paragraph_font(section2_heading, font_size=14)
    
    scope_para = doc.add_paragraph()
    stats = calculate_statistics(data)
    scope_para.add_run(f"T·∫≠p trung v√†o lƒ©nh v·ª±c {data.get('industry', 'N/A')} t·∫°i th·ªã tr∆∞·ªùng {data.get('market', 'Vi·ªát Nam')} trong nƒÉm {datetime.now().year}, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p nghi√™n c·ª©u ph√¢n t√≠ch ƒëa t·∫ßng v·ªõi {stats['total_questions']} c√¢u h·ªèi nghi√™n c·ª©u, √°p d·ª•ng AI ƒë·ªÉ thu th·∫≠p v√† ph√¢n t√≠ch th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau.")
    scope_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(scope_para)
    
    # 3. Ph√°t hi·ªán ch√≠nh
    section3_heading = doc.add_heading('3. üîç PH√ÅT HI·ªÜN CH√çNH', level=2)
    set_paragraph_font(section3_heading, font_size=14)
    
    # Extract key insights from research content
    insights = extract_key_insights_for_summary(data)
    
    findings_para = doc.add_paragraph()
    findings_para.add_run("Ch√∫ng t√¥i nh·∫≠n th·∫•y r·∫±ng:")
    for i, insight in enumerate(insights[:4], 1):  # Top 4 insights for findings
        # Capitalize first letter of insight
        insight_text = insight['insight']
        if insight_text and len(insight_text) > 0:
            insight_text = insight_text[0].upper() + insight_text[1:] if len(insight_text) > 1 else insight_text.upper()
        findings_para.add_run(f"\n‚Ä¢ {insight_text}")
    
    findings_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(findings_para)
    
    # 4. ƒê·ªÅ xu·∫•t h√†nh ƒë·ªông
    section4_heading = doc.add_heading('4. üöÄ ƒê·ªÄ XU·∫§T H√ÄNH ƒê·ªòNG', level=2)
    set_paragraph_font(section4_heading, font_size=14)
    
    action_para = doc.add_paragraph()
    action_para.add_run("Ch√∫ng t√¥i ƒë·ªÅ xu·∫•t doanh nghi·ªáp n√™n:")
    action_para.add_run(f"""
‚Ä¢ T·∫≠p trung ph√°t tri·ªÉn c√°c s·∫£n ph·∫©m/d·ªãch v·ª• ph√π h·ª£p v·ªõi xu h∆∞·ªõng th·ªã tr∆∞·ªùng hi·ªán t·∫°i
‚Ä¢ X√¢y d·ª±ng chi·∫øn l∆∞·ª£c ti·∫øp th·ªã v√† b√°n h√†ng d·ª±a tr√™n insights t·ª´ nghi√™n c·ª©u
‚Ä¢ ƒê·∫ßu t∆∞ v√†o c√¥ng ngh·ªá v√† ƒë·ªïi m·ªõi ƒë·ªÉ n√¢ng cao nƒÉng l·ª±c c·∫°nh tranh
‚Ä¢ TƒÉng c∆∞·ªùng h·ª£p t√°c v·ªõi c√°c ƒë·ªëi t√°c chi·∫øn l∆∞·ª£c trong ng√†nh
‚Ä¢ X√¢y d·ª±ng h·ªá th·ªëng theo d√µi v√† ƒë√°nh gi√° th∆∞·ªùng xuy√™n ƒë·ªÉ ·ª©ng ph√≥ v·ªõi thay ƒë·ªïi th·ªã tr∆∞·ªùng""")
    
    action_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(action_para)
    
    # 5. T√°c ƒë·ªông k·ª≥ v·ªçng
    section5_heading = doc.add_heading('5. üìà T√ÅC ƒê·ªòNG K·ª≤ V·ªåNG', level=2)
    set_paragraph_font(section5_heading, font_size=14)
    
    impact_para = doc.add_paragraph()
    impact_para.add_run(f"ƒêi·ªÅu n√†y s·∫Ω d·∫´n ƒë·∫øn vi·ªác n√¢ng cao v·ªã th·∫ø c·∫°nh tranh c·ªßa doanh nghi·ªáp trong ng√†nh {data.get('industry', 'N/A')}, tƒÉng c∆∞·ªùng kh·∫£ nƒÉng th√≠ch ·ª©ng v·ªõi thay ƒë·ªïi th·ªã tr∆∞·ªùng, v√† t·ªëi ∆∞u h√≥a hi·ªáu qu·∫£ kinh doanh. D·ª± ki·∫øn s·∫Ω c·∫£i thi·ªán ƒë√°ng k·ªÉ kh·∫£ nƒÉng ra quy·∫øt ƒë·ªãnh chi·∫øn l∆∞·ª£c v√† t·∫°o ra l·ª£i th·∫ø c·∫°nh tranh b·ªÅn v·ªØng trong m√¥i tr∆∞·ªùng kinh doanh nƒÉng ƒë·ªông.")
    impact_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    set_paragraph_font(impact_para)
    
    # Footer info
    doc.add_paragraph()
    footer_para = doc.add_paragraph()
    footer_para.add_run('üìÖ B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi Market Research Automation System').italic = True
    footer_para.add_run(f'\n‚è∞ Ng√†y t·∫°o: {datetime.now().strftime("%d/%m/%Y %H:%M")}')
    footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(footer_para, font_size=9)

def extract_key_insights_for_summary(data):
    """Tr√≠ch xu·∫•t key insights ƒë·ªÉ t·∫°o executive summary ph·∫ßn ph√°t hi·ªán ch√≠nh"""
    insights = []
    
    for layer in data.get('research_results', []):
        layer_name = layer.get('layer_name', '')
        
        for category in layer.get('categories', []):
            category_name = category.get('category_name', '')
            
            for question in category.get('questions', []):
                # L·∫•y t·ª´ comprehensive report tr∆∞·ªõc
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    content = layer4_comprehensive.get('comprehensive_content', '')
                    if content:
                        # Extract key trends and opportunities
                        sentences = content.split('.')
                        for sentence in sentences:
                            if any(keyword in sentence.lower() for keyword in ['xu h∆∞·ªõng', 'c∆° h·ªôi', 'th√°ch th·ª©c', 'tƒÉng tr∆∞·ªüng', 'ph√°t tri·ªÉn']):
                                insight = sentence.strip()
                                if len(insight) > 50:  # Only meaningful insights
                                    insights.append({
                                        'category': f"{layer_name} - {category_name}",
                                        'insight': insight[:150] + "..." if len(insight) > 150 else insight
                                    })
                                    break
                # Fallback to layer 3
                elif question.get('layer3_content'):
                    content = question.get('layer3_content', '')
                    sentences = content.split('.')
                    for sentence in sentences:
                        if any(keyword in sentence.lower() for keyword in ['quan tr·ªçng', 'ch√≠nh', 'ƒë√°ng ch√∫ √Ω', 'n·ªïi b·∫≠t']):
                            insight = sentence.strip()
                            if len(insight) > 50:
                                insights.append({
                                    'category': f"{layer_name} - {category_name}",
                                    'insight': insight[:120] + "..." if len(insight) > 120 else insight
                                })
                                break
    
    return insights[:6]  # Top 6 insights for summary

def clean_comprehensive_content(content):
    """Remove numbering, section headers, and intro sentences from comprehensive content"""
    if not content:
        return content
    
    # Remove intro sentences like "ƒê·ªÉ tr·∫£ l·ªùi chuy√™n s√¢u c√¢u h·ªèi v·ªÅ..."
    cleaned_content = re.sub(r'^ƒê·ªÉ tr·∫£ l·ªùi[^,]*,\s*', '', content, flags=re.MULTILINE)
    cleaned_content = re.sub(r'^ƒê·ªÉ tr·∫£ l·ªùi[^.]*\.\s*', '', cleaned_content, flags=re.MULTILINE)
    cleaned_content = re.sub(r'^Nh·∫±m tr·∫£ l·ªùi[^,]*,\s*', '', cleaned_content, flags=re.MULTILINE)
    cleaned_content = re.sub(r'^Nh·∫±m ph√¢n t√≠ch[^,]*,\s*', '', cleaned_content, flags=re.MULTILINE)
    
    # Remove section headers like "üìä PH√ÇN T√çCH HI·ªÜN TR·∫†NG:", "‚ö° DRIVERS & IMPACTS:", etc.
    cleaned_content = re.sub(r'\n*[üìä‚ö°üìà‚ö†Ô∏èüöÄ]\s*[A-Z√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ç√å·ªàƒ®·ªä√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥\s&]+:\s*', '\n\n', cleaned_content)
    
    # Remove numbered sections like "1. SECTION:", "2. ANALYSIS:", etc.
    cleaned_content = re.sub(r'\n*\d+\.\s+[A-Z√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ç√å·ªàƒ®·ªä√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥\s]+:\s*', '\n\n', cleaned_content)
    
    # Remove **bold headers** at start of lines
    cleaned_content = re.sub(r'\n\*\*[^*]+\*\*\s*\(\d+-\d+\s+t·ª´\)\s*\n', '\n\n', cleaned_content)
    
    # Remove intro phrases at the beginning
    intro_patterns = [
        r'^Trong b·ªëi c·∫£nh n√†y,\s*',
        r'^Ch√∫ng ta c·∫ßn xem x√©t[^.]*\.\s*',
        r'^Vi·ªác ph√¢n t√≠ch[^.]*\.\s*'
    ]
    
    for pattern in intro_patterns:
        cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.MULTILINE)
    
    # Clean up extra newlines
    cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)
    
    # Remove leading/trailing whitespace
    cleaned_content = cleaned_content.strip()
    
    return cleaned_content

def create_comprehensive_word_report(json_file, output_file=None):
    """
    Export layered research results to Word document
    H·ªó tr·ª£ comprehensive Layer 4 reports with enhanced formatting
    """
    
    if not os.path.exists(json_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {json_file}")
        return None
    
    # Load data
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # T·∫°o output filename n·∫øu ch∆∞a c√≥
    if output_file is None:
        base_name = os.path.splitext(json_file)[0]
        output_file = f"{base_name}_comprehensive_report.docx"
    
    # Create document
    doc = Document()
    add_custom_styles(doc)
    
    # ===== TRANG B√åA =====
    print("üìÑ T·∫°o trang b√¨a...")
    
    title = doc.add_heading('B√ÅO C√ÅO NGHI√äN C·ª®U TH·ªä TR∆Ø·ªúNG', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(title, font_size=18)
    
    doc.add_paragraph()
    
    # Th√¥ng tin c∆° b·∫£n v·ªõi Table
    info_heading = doc.add_heading('üìä TH√îNG TIN T·ªîNG QUAN', level=2)
    info_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(info_heading, font_size=14)
    
    create_info_table(doc, data)
    
    doc.add_paragraph()
    
    # Purpose
    if data.get('purpose'):
        purpose_heading = doc.add_heading('üéØ M·ª§C ƒê√çCH NGHI√äN C·ª®U', level=2)
        purpose_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_paragraph_font(purpose_heading, font_size=14)
        
        purpose_para = doc.add_paragraph(data['purpose'])
        purpose_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        set_paragraph_font(purpose_para)
    
    doc.add_page_break()
    
    # ===== N·ªòI DUNG CH√çNH =====
    print("üîç T·∫°o n·ªôi dung b√°o c√°o chi ti·∫øt...")
    
    results_heading = doc.add_heading('üìã K·∫æT QU·∫¢ NGHI√äN C·ª®U CHI TI·∫æT', level=1)
    results_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER
    set_paragraph_font(results_heading, font_size=16)
    
    for layer_idx, layer in enumerate(data.get('research_results', []), 1):
        layer_name = layer.get('layer_name', '')
        
        print(f"  üî• X·ª≠ l√Ω Layer: {layer_name}")
        
        # Layer heading
        layer_heading = doc.add_heading(f"{layer_idx}. {layer_name.upper()}", level=1)
        set_paragraph_font(layer_heading, font_size=16)
        
        for cat_idx, category in enumerate(layer.get('categories', []), 1):
            category_name = category.get('category_name', '')
            
            print(f"    üìã Category: {category_name}")
            
            # Category heading
            category_heading = doc.add_heading(f"{cat_idx}. {category_name}", level=2)
            set_paragraph_font(category_heading, font_size=14)
            
            for q_idx, question in enumerate(category.get('questions', []), 1):
                main_question = question.get('main_question', '')
                
                print(f"      ‚ùì Question: {main_question[:50]}...")
                
                # Main question heading
                question_heading = doc.add_heading(f"{q_idx}. {main_question}", level=3)
                set_paragraph_font(question_heading, font_size=12)
                
                # Layer 3 content (more concise)
                layer3_content = question.get('layer3_content', '')
                if layer3_content:
                    # Clean up and make more concise
                    cleaned_content = layer3_content.strip()
                    
                    layer3_para = doc.add_paragraph()
                    layer3_para.add_run('üìã PH√ÇN T√çCH T·ªîNG QUAN:').bold = True
                    layer3_para.add_run('\n' + cleaned_content)
                    layer3_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                    set_paragraph_font(layer3_para)
                
                # Layer 4 comprehensive report (∆∞u ti√™n)
                layer4_comprehensive = question.get('layer4_comprehensive_report', {})
                if layer4_comprehensive:
                    print(f"        üîç C√≥ comprehensive report")
                    
                    comprehensive_content = layer4_comprehensive.get('comprehensive_content', '')
                    if comprehensive_content:
                        # Clean up content and make more structured
                        cleaned_comp_content = clean_comprehensive_content(comprehensive_content).strip()
                        
                        comp_para = doc.add_paragraph()
                        comp_para.add_run('üéØ PH√ÇN T√çCH CHUY√äN S√ÇU:').bold = True
                        comp_para.add_run('\n' + cleaned_comp_content)
                        comp_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                        set_paragraph_font(comp_para)
                    
                    # Timestamp
                    timestamp = layer4_comprehensive.get('enhancement_timestamp', '')
                    if timestamp:
                        time_para = doc.add_paragraph()
                        time_para.add_run(f"‚è∞ C·∫≠p nh·∫≠t: {timestamp}").italic = True
                        time_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
                        set_paragraph_font(time_para, font_size=9)
                
                # Layer 4 individual enhancements (fallback)
                elif question.get('layer4_enhancements', {}):
                    layer4_enhancements = question.get('layer4_enhancements', {})
                    print(f"        üéØ C√≥ {len(layer4_enhancements)} individual enhancements")
                    
                    layer4_heading = doc.add_paragraph()
                    layer4_heading.add_run('üéØ PH√ÇN T√çCH CHI TI·∫æT:').bold = True
                    
                    for sub_question, enhancement_data in layer4_enhancements.items():
                        # Sub-question
                        sub_q_para = doc.add_paragraph()
                        sub_q_para.add_run(f"‚Ä¢ {sub_question}").bold = True
                        set_paragraph_font(sub_q_para)
                        
                        # Enhanced content
                        enhanced_content = enhancement_data.get('enhanced_content', '')
                        if enhanced_content:
                            enh_para = doc.add_paragraph(enhanced_content)
                            enh_para.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                            enh_para.paragraph_format.left_indent = Inches(0.3)
                            set_paragraph_font(enh_para)
                        
                        # Timestamp
                        timestamp = enhancement_data.get('enhancement_timestamp', '')
                        if timestamp:
                            time_para = doc.add_paragraph()
                            time_para.add_run(f"‚è∞ C·∫≠p nh·∫≠t: {timestamp}").italic = True
                            time_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
                            set_paragraph_font(time_para, font_size=9)
    
    # ===== REFERENCES SECTION =====
    create_references_section(doc, data)
    
    # ===== EXECUTIVE SUMMARY =====
    create_executive_summary(doc, data)
    
    # Save document
    print(f"üíæ L∆∞u file: {output_file}")
    doc.save(output_file)
    print(f"‚úÖ ƒê√£ t·∫°o Word document: {output_file}")
    
    return output_file

def calculate_statistics(data):
    """T√≠nh to√°n th·ªëng k√™ cho b√°o c√°o"""
    stats = {
        'total_layers': 0,
        'total_categories': 0,
        'total_questions': 0,
        'comprehensive_reports': 0,
        'individual_enhancements': 0
    }
    
    research_results = data.get('research_results', [])
    stats['total_layers'] = len(research_results)
    
    for layer in research_results:
        categories = layer.get('categories', [])
        stats['total_categories'] += len(categories)
        
        for category in categories:
            questions = category.get('questions', [])
            stats['total_questions'] += len(questions)
            
            for question in questions:
                # Count comprehensive reports
                if question.get('layer4_comprehensive_report'):
                    stats['comprehensive_reports'] += 1
                
                # Count individual enhancements
                individual_enhancements = question.get('layer4_enhancements', {})
                stats['individual_enhancements'] += len(individual_enhancements)
    
    return stats

def find_latest_research_file():
    """T√¨m file research m·ªõi nh·∫•t trong th∆∞ m·ª•c output"""
    output_dir = 'output'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    files = []
    for file in os.listdir(output_dir):
        if file.startswith('layer3_research_') and file.endswith('.json'):
            files.append(os.path.join(output_dir, file))
    
    if not files:
        # Fallback: t√¨m trong th∆∞ m·ª•c g·ªëc
        files = [f for f in os.listdir('.') if f.startswith('layer3_research_') and f.endswith('.json')]
        
    if not files:
        return None
    
    # S·∫Øp x·∫øp theo th·ªùi gian modification
    files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    return files[0]

def main():
    """Main function"""
    print("üî¨ COMPREHENSIVE WORD EXPORT TOOL")
    print("="*50)
    
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
    output_dir = 'output'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c: {output_dir}")
    
    # T√¨m file research
    json_file = find_latest_research_file()
    
    if not json_file:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file research results!")
        print("üí° H√£y ch·∫°y Layer 3 research tr∆∞·ªõc.")
        return
    
    print(f"üìÅ S·ª≠ d·ª•ng file: {json_file}")
    
    # T·∫°o output filename trong th∆∞ m·ª•c output
    base_name = os.path.basename(json_file).replace('layer3_research_', '').replace('.json', '')
    output_file = os.path.join(output_dir, f"B√°o_c√°o_nghi√™n_c·ª©u_th·ªã_tr∆∞·ªùng_{base_name}.docx")
    
    # Export
    result = create_comprehensive_word_report(json_file, output_file)
    
    if result:
        print(f"\nüéâ HO√ÄN TH√ÄNH!")
        print(f"üìÑ File Word: {result}")
        print(f"üí° C√≥ th·ªÉ m·ªü file b·∫±ng: open \"{result}\"")
    else:
        print("‚ùå Export th·∫•t b·∫°i!")

def format_purpose_text(purpose_text):
    """Format purpose text to ensure proper capitalization for both bullet and dash formats"""
    if not purpose_text:
        return purpose_text
    
    # Split by common delimiters and capitalize each item
    lines = purpose_text.split('\n')
    formatted_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Handle both bullet (‚Ä¢) and dash (-) formats
        if line.startswith('‚Ä¢') or line.startswith('-'):
            # Extract the text after the bullet/dash
            prefix = line[0]  # ‚Ä¢ or -
            text = line[1:].strip()
            if text:
                # Capitalize first letter
                text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()
                formatted_lines.append(f"{prefix} {text}")
        else:
            # Regular text - capitalize first letter
            if line:
                line = line[0].upper() + line[1:] if len(line) > 1 else line.upper()
                formatted_lines.append(line)
    
    return '\n'.join(formatted_lines)

if __name__ == "__main__":
    main() 